Great — thanks for the clarification.

Here’s the **updated clean version** based on your requirements:

* All model types are `"HAS"`.
* `orig_products` is the same as `product`.
* No need to track actual model class like LogisticRegression or LGBM.

---

### ✅ Final Code:

```python
import os
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

def X_data_new(n_samples, n_features):
    X = np.random.rand(n_samples, n_features)
    y = np.random.randint(0, 2, size=n_samples)
    return X, y

models_base_path = "../notebooks/models/HAS"
output_preds = []

# Walk through all subfolders
for root, _, files in os.walk(models_base_path):
    for file in files:
        if file == "trained_model.pkl":
            model_path = os.path.join(root, file)
            try:
                # Extract product from path (e.g., ca_ach)
                product = root.split("model_data/HAS/")[-1].split("/")[0]
                country = product.split("_")[0].upper()

                # Load model
                model_object = pd.read_pickle(model_path)

                # Get feature count
                if isinstance(model_object, LogisticRegression):
                    n_features = model_object.coef_.shape[1]
                elif isinstance(model_object, lgb.LGBMClassifier):
                    n_features = model_object.booster_.num_feature()
                else:
                    print(f"Unknown model type at {model_path}")
                    continue

                # Generate synthetic data and predict
                X_data, y_data = X_data_new(n_samples=100, n_features=n_features)
                preds = model_object.predict_proba(X_data)[:, 1]

                # Create result DataFrame
                pred_df = pd.DataFrame({
                    "uen": np.random.randint(100000, 999999, size=100),
                    "product": product,
                    "country": country,
                    "model_type": "HAS",
                    "propensity": preds,
                    "orig_products": product
                })

                output_preds.append(pred_df)
                print(f"✓ Processed: {product} model from {model_path}")

            except Exception as e:
                print(f"✗ Failed to process {model_path}: {e}")

# Concatenate all predictions
final_output_df = pd.concat(output_preds, ignore_index=True)

# Optional: Save to CSV
# final_output_df.to_csv("all_model_predictions.csv", index=False)
```

---

Let me know if you want to:

* Add timestamp or version tagging
* Include SHAP values or additional metrics
* Push this to S3 from within your notebook
